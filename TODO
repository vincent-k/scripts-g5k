---
TODO
---

Au fur et à mesure, rajouter des '-' pour des nouveaux items à considerer.
Remplacer par un "OK" quand l'item est adressé, "KO" quand ca n'a pas de sens (et justifier).

Concernant les VMs:

OK Augmentation de la taille de l'image (au moins 5Go ?). Garder de petites images (2/3 Go pour tests rapide)
OK déclaration des proxys dans les VMs:
OK installation d'une workload dans les VMs:
  OK environnement graphique => On considère des VMs desktops pour le dev.
  OK env. de compilation C
  OK utilisation de handbrake comme workload
  OK creation d'un script pour recuperer le framerate (encodage video via handbrake)
  OK (penser à un/deux scripts qui lance la workload sur une VM donnée si besoin)
OK (ajustable) Augmenter la conso mémoire des VMs 2Go ?
OK (ajustable) Peut-être 2 vCPUs également (avec affinité pCPU/vCPU) pour tester d'autres configurations.
- Mapper 1 vcpu par pcpu :  cpuaffinity dans libvirt (virsh vcpupin)

Concernant l'expérimentation:
OK Rajouter le boot du noeud destination (kaboot ? & co)
OK Rajouter l'extinction du noeud source une fois vide
OK Noter les temps de migration moyens
OK Noter les temps de boot des noeuds
OK Integrer le workload et recuperer automatiquement les resultats
OK Acceder à la BMC pour avoir la conso électrique ? (cluster edel)
OK Utiliser un serveur NFS (noeud dedie) au cas ou storage5k n'est pas disponible
OK Utiliser le lien infiniband (si disponible) pour le montage du partage NFS (interface ib0)
OK Installer ipmitools (+freeipmi) sur l'img des noeuds
OK Resoudre le probleme de la configuration reseau des noeuds => ecrasement du fichier 'interfaces' < SOLUTION : suppression de rc.local apres le premier boot + penser a desactiver le stp
OK Creer un script qui capture et renvoie la consommation d'energie en Watt
OK Rendre les configurations des noeuds persistantes apres reboot : config_infiniband + mount_nfs_storage
OK Script pour test de charge d'un noeud
OK Reset d'une expe sans redeploiement
OK Mettre les images de backing hors de la RAM
OK Plotter les resultats du workload apache
- Analyser le bench complet d'un noeud (workload stats/summary + consumption)
- Verifier la coherence des 'pourcentages de req' en fonction de la duree du workload (comparer long et court terme sans migration)
- Trouver le 'nb de req concurrente' ideal pour le bench d'apache (bruteforce from 100) ?? cibler les migrations intra-rack! pour pas faire nimp niveau reseau/BP ++ lancer pdt la nuit pour limiter les transferts concurrents
- Ajouter la possibilite de demander un switch unique pour un nombre de noeud defini ("nodes=x/switch=1")
- Tester les decommissioning inter-racks sur edel et analyser la BP reseau dispo par rack => trouver le 'nb de req concurrente' ideal
- Rendre l'ensemble des scripts plus autonomes: detection d'erreurs Noeud/VM -> corrections -> redeploiement/repreparation ..

- Mettre en place une experimentation de type : [x * 1 cpu nodes > 1 * x cpu node]
- Faire fonctionner NFS/RDMA
- Experimentations de plus grande taille pour l'expe finale (equivalent d'un "plusieurs clusters")
- Faire des migrations "asymmétrique": pour le moment on d'un noeud à un noeud. Dans la pratique, si on branche des noeuds plus récents, ils stockeront plus que le contenu d'un ancien noeud. L'équivalent de 1.5 ou de 2 anciens noeuds par exemple.
- On peut faire des evals à petite échelles pour tester rapidement des ordonnancements différent. Faire une grosse eval de temps en temps permet juste de s'assurer le plus tôt possible que l'environnement est toujours ok.
- Test de la saturation inter-link Nancy => Attention dimensionnement
